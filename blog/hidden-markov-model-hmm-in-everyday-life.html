<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="next-head-count" content="2"/><link rel="preload" href="/_next/static/css/07d33796bb7e7edb.css" as="style"/><link rel="stylesheet" href="/_next/static/css/07d33796bb7e7edb.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-514908bffb652963.js" defer=""></script><script src="/_next/static/chunks/framework-91d7f78b5b4003c8.js" defer=""></script><script src="/_next/static/chunks/main-e47bb435dabe4202.js" defer=""></script><script src="/_next/static/chunks/pages/_app-0a43cca45a13548b.js" defer=""></script><script src="/_next/static/chunks/963-7455877b8d482f59.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-484b6f68720fef9b.js" defer=""></script><script src="/_next/static/Y66DXjpn3XfY2iHDxlRI4/_buildManifest.js" defer=""></script><script src="/_next/static/Y66DXjpn3XfY2iHDxlRI4/_ssgManifest.js" defer=""></script><script src="/_next/static/Y66DXjpn3XfY2iHDxlRI4/_middlewareManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><main class=""><div class="mx-auto max-w-3xl h-full"><a class="btn btn-back" href="/">Go Back</a><div class="card card-page"><h1 class="post-title">Hidden Markov Model(HMM) in everyday life</h1><div class="post-date">Posted on <!-- -->March 10th, 2019</div><img src="https://github.com/datduyng/datduyng.github.io/blob/2170af7581dc2fe4fd411549f84ae1785671faaa/assets/img/post/2019-03-10-hmm-graphical.png?raw=true" alt=""/><div class="post-body"><div><p><strong>Section</strong><br> 
<strong>I. Introduction</strong><br> 
&nbsp; 1. Baysian Network<br> 
&nbsp; 2. HMM<br> 
&nbsp; 3. Sumary of HMM Applications<br> 
<strong>II. Tools set for HMM</strong><br> 
<strong>III. HMM Applications in details</strong><br> 
&nbsp; 1. Speech tagging<br> 
&nbsp; 2. Producitivities tracker<br> 
<strong>III. Conclusion</strong><br> 
<strong>IV. References</strong><br> </p>
<h2 id="i-introduction">I. Introduction</h2>
<p>Sequential predictive model are seen in most technology now aday. Many successful sequential predictive model like RNN or LSTM are builded up by one of the most influential predictive model, Hidden Markov Model(HMM). It is important to learn about HMM as it is a foundation for many other machine learning model. For me, Learning HMM is like learning to write machine code or C-code as there are no magic behind any derivation once I understand it. This article will give an introduction to HMM and quick information on some HMM applicaiton in everyday life. With that being said, Let&#39;s jump right in.</p>
<h3 id="1-baysian-network">1. Baysian Network</h3>
<p>Welcome, I assume you familiar with term like state machine and markov properties. one often hear these word in many of my computer science course. They often ring a bell and remind me of the word &quot;Bayesian&quot;.  So let&#39;s shift gear for a paragraph to get an intutition behind baysian network. </p>
<p>In one sentence, Bayesian network is a type of datastructure that describe the dependency of a <strong>directed acyclic graph</strong> via probabilistic edges. Baysian network aim to model conditional dependency by representing its as edges in the graph. There are three type of dependency in Baysian network as shown below: <br> </p>
<center><img src="https://user-images.githubusercontent.com/35666615/53527326-46e96e80-3aac-11e9-9dd9-14195f10f099.PNG" height="330" width="650"> </center>
<center> Type of dependency in baysian network <br> (adapted from cs228 notes)</center>

<h3 id="2-hidden-markov-model-hmm">2. Hidden Markov Model (HMM)</h3>
<p>Hidden Markov Model(HMM) is a special type of bayesian network. <strong>First order hidden markov</strong> is a combination of case a and b.  A graphical model of HMM is shown below. <br> </p>
<center><img src="https://user-images.githubusercontent.com/35666615/53526922-1e14a980-3aab-11e9-8eb9-15da1997c1c6.PNG" height="330" width="650"></center> 
<center> Graphical model of HMM</center>

<p>Each edge is represent as an inference from one node to other node. For example, edge from Node <code>S1</code> to <code>S2</code> describe inference from <code>S1</code> to <code>S2</code>. In the Bayesian world, this is denoted as P(S2|S1) or probaility of S2 given the state at S1. To wrap the Bayesian business up, we can compute the inference using Bayesian rule as follow</p>
<p>$$
P(A|B) = \frac{P(B|A) P(A)}{P(B)}
$$</p>
<p>where </p>
<ul>
<li>$$P(A\mid B)$$: probability of state A given probability of B. </li>
<li>$$P(B\mid A)$$: probability of state B given probability of A. </li>
<li>$$P(B)$$: Probaility of A being true( the given probability)</li>
<li>$$P(A)$$: Probability of B being true (the given probability)</li>
</ul>
<p>These <a href="https://ermongroup.github.io/cs228-notes/representation/directed/">notes</a> from cs228 course explain more in-depth on Baysian network.</p>
<p>When dealing with HMM, each observable state will depend on one or multiple hidden state. </p>
<ul>
<li>here, hidden state refer to state that are unseen or <strong>hidden</strong>. As an example, hidden state of a human being can be happy or sad. </li>
<li>observable state refering to state that can be seen or <strong>observable</strong>. For example, Crying or smiling</li>
</ul>
<h3 id="3-sumary-of-hmm-applications">3. Sumary of HMM Applications</h3>
<p>enough for the theory, let&#39;s now go over an sumarry of HMM in everyday actions. </p>
<ul>
<li>Weather prediction</li>
<li>Speech recognition</li>
<li>Part of speech tagging</li>
<li>Music generation</li>
<li>Google search</li>
<li>Word type correction</li>
<li>Digital writing correction</li>
<li>Facial expression identification using in video</li>
<li>Human action recognition from Time Sequential Images</li>
<li>Biological Sequence Analysis.<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2766791/">link</a></li>
<li>Creadit card fraud detection. <a href="https://thesai.org/Downloads/Volume7No2/Paper_5-Hidden_Markov_Models_HMMs_and_Security_Applications.pdf">Link</a></li>
<li>Filtering Sensor values.(Kalman filter)</li>
</ul>
<h2 id="ii-tools-set-for-hmm">II. Tools set for HMM</h2>
<p>When working with HMM, we will need some knowledge about the problem domain. As shown in Bayse&#39;s rule, to compute the <strong>posterior</strong>, $$P(A|B)$$ we need a <strong>prior</strong>, $$P(A)$$,  knowledge about the problem domain as well as the likelihood of the state in the sequence. 
In HMM we will need Transition matrix(A), Emission matrix(B), and initial state matrix($\pi$). </p>
<ul>
<li>Transition matrix($A$): represent the probability of transitioning from a hidden state to other hidden state. </li>
<li>Emission matrix($B$): indicate the probability of an <strong>emitted</strong> being at a hidden state. </li>
<li>Initial state matrix($$\pi$$): indicate the starting hidden state. As when we start a sequence, there are no previous state to look at.</li>
</ul>
<p>The three matrix weight can be represent on graphical HMM model as below</p>
<center><img src="https://user-images.githubusercontent.com/35666615/53783262-7ea24d00-3ed6-11e9-8a93-92c5e400cf5c.png" height="330" width="650"></center> 
<center> Graphical model of HMM</center>


<p>To generalize, what we just discussed. </p>
<ul>
<li>$$A_{i,j}$$ indicate the transition state from one hidden state,i to next hidden state, j.</li>
<li>$$B_{i}(O_t)$$indicate the emission probability of emitting an observable state $$O_t$$ from hidden state at step t, denotes as i.</li>
<li>$$\pi_i$$ indicate the initial probability of a hidden state, i.</li>
</ul>
<h2 id="iii-hmm-aplications-hmm-in-actions">III. HMM Aplications (HMM in actions)</h2>
<p>There are far more and beyond application utilizing HMM technique. Let&#39;s discuss a couple to gain a more innovative look at HMM. </p>
<ul>
<li><p>Weather prediction</p>
<ul>
<li>Weather data source are gigantic as we can collect everyday. Well, I would argu that HMM cannot be a fully weather forecast predictor, but It can augment with other technique create by brillian minds meteorologist.</li>
</ul>
</li>
<li><p>Speech recognition</p>
<ul>
<li>Our language often follow a pattern. For example, a verb are more likely to come up after a noun or a noun often come after the word &#39;the&#39;. Our speech often are represented as wave form signal. After sampling, we can retrieve discrete point which we can apply the HMM technique that we discuss above to analyze.</li>
</ul>
</li>
<li><p>Part of speech tagging</p>
<ul>
<li>From young age, we often start learning our language by breaking and analyzing part of speech. As part of the process we assigning if a sub-sequence is a verb or a noun, we often find pattern on what part of speech usually come after another. This task can be also accomplish with HMM. In Divya&#39; <a href="https://medium.freecodecamp.org/an-introduction-to-part-of-speech-tagging-and-the-hidden-markov-model-953d45338f24">article</a>, she discussed over part of speech using HMM amazingly well.</li>
</ul>
</li>
<li><p>Music generation</p>
</li>
<li><p>Google search</p>
<ul>
<li>Of course, Our everyday favorite tool is base on a HMM model. Imagine searching for needles(our queries) in the world biggest haystack(the world wide web). The search would become exhaustive search. Luckily, Google came up with a way of assigning probability to link using <a href="http://pr.efactory.de/e-pagerank-algorithm.shtml">random surfer trick</a>. The first chapter of John Maccormick&#39;s book, 9 algorithm that change the future also walk in-depth over the PageRank algorithm.</li>
</ul>
</li>
<li><p>Word type correction</p>
<ul>
<li>Similar to speech recognition, on each word that user type, HMM model can also help predict the word that are likely to appear next, thus, this will help display option for user correct their writing.</li>
</ul>
</li>
<li><p>Digital writing correction</p>
</li>
<li><p>Facial expression identification using in video</p>
</li>
<li><p>Human action recognition from Time Sequential Images</p>
</li>
<li><p>Biological Sequence Analysis.<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2766791/">link</a></p>
</li>
<li><p>Creadit card fraud detection. <a href="https://thesai.org/Downloads/Volume7No2/Paper_5-Hidden_Markov_Models_HMMs_and_Security_Applications.pdf">Link</a></p>
</li>
<li><p>Filtering Sensor values.(Kalman filter)</p>
<ul>
<li>One of my favorite application of HMM is the kalman filter. In discrete time, we have HMM. In the continuous world, we have the kalman filter. Kalman filter is mostly use to verify and extract sensory data. Kalman filter assign probability to a sensor value thus, help us tell how much to trust a sensor values.</li>
</ul>
</li>
</ul>
<h2 id="iv-references">IV. References</h2>
<p><a href="https://ermongroup.github.io/cs228-notes/representation/directed/">https://ermongroup.github.io/cs228-notes/representation/directed/</a>
<a href="http://www.davidsbatista.net/blog/2017/11/11/HHM_and_Naive_Bayes/">http://www.davidsbatista.net/blog/2017/11/11/HHM_and_Naive_Bayes/</a>
<a href="http://www.cs.umd.edu/~djacobs/CMSC828/ApplicationsHMMs.pdf">http://www.cs.umd.edu/~djacobs/CMSC828/ApplicationsHMMs.pdf</a>
<a href="https://medium.com/@postsanjay/hidden-markov-models-simplified-c3f58728caab">https://medium.com/@postsanjay/hidden-markov-models-simplified-c3f58728caab</a></p>
</div></div></div></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"title":"Hidden Markov Model(HMM) in everyday life","date":"March 10th, 2019","excerpt":"Building your own weather station with Arduino family micro controller","cover_image":"https://github.com/datduyng/datduyng.github.io/blob/2170af7581dc2fe4fd411549f84ae1785671faaa/assets/img/post/2019-03-10-hmm-graphical.png?raw=true"},"slug":"hidden-markov-model-hmm-in-everyday-life","content":"\n\n**Section**\u003cbr\u003e \n**I. Introduction**\u003cbr\u003e \n\u0026nbsp; 1. Baysian Network\u003cbr\u003e \n\u0026nbsp; 2. HMM\u003cbr\u003e \n\u0026nbsp; 3. Sumary of HMM Applications\u003cbr\u003e \n**II. Tools set for HMM**\u003cbr\u003e \n**III. HMM Applications in details**\u003cbr\u003e \n\u0026nbsp; 1. Speech tagging\u003cbr\u003e \n\u0026nbsp; 2. Producitivities tracker\u003cbr\u003e \n**III. Conclusion**\u003cbr\u003e \n**IV. References**\u003cbr\u003e \n\n\n## I. Introduction\n\nSequential predictive model are seen in most technology now aday. Many successful sequential predictive model like RNN or LSTM are builded up by one of the most influential predictive model, Hidden Markov Model(HMM). It is important to learn about HMM as it is a foundation for many other machine learning model. For me, Learning HMM is like learning to write machine code or C-code as there are no magic behind any derivation once I understand it. This article will give an introduction to HMM and quick information on some HMM applicaiton in everyday life. With that being said, Let's jump right in.\n### 1. Baysian Network\nWelcome, I assume you familiar with term like state machine and markov properties. one often hear these word in many of my computer science course. They often ring a bell and remind me of the word \"Bayesian\".  So let's shift gear for a paragraph to get an intutition behind baysian network. \n\nIn one sentence, Bayesian network is a type of datastructure that describe the dependency of a **directed acyclic graph** via probabilistic edges. Baysian network aim to model conditional dependency by representing its as edges in the graph. There are three type of dependency in Baysian network as shown below: \u003cbr\u003e \n\u003ccenter\u003e\u003cimg src=\"https://user-images.githubusercontent.com/35666615/53527326-46e96e80-3aac-11e9-9dd9-14195f10f099.PNG\" height=\"330\" width=\"650\"\u003e \u003c/center\u003e\n\u003ccenter\u003e Type of dependency in baysian network \u003cbr\u003e (adapted from cs228 notes)\u003c/center\u003e\n\n### 2. Hidden Markov Model (HMM)\nHidden Markov Model(HMM) is a special type of bayesian network. **First order hidden markov** is a combination of case a and b.  A graphical model of HMM is shown below. \u003cbr\u003e \n\n\u003ccenter\u003e\u003cimg src=\"https://user-images.githubusercontent.com/35666615/53526922-1e14a980-3aab-11e9-8eb9-15da1997c1c6.PNG\" height=\"330\" width=\"650\"\u003e\u003c/center\u003e \n\u003ccenter\u003e Graphical model of HMM\u003c/center\u003e\n\nEach edge is represent as an inference from one node to other node. For example, edge from Node `S1` to `S2` describe inference from `S1` to `S2`. In the Bayesian world, this is denoted as P(S2\\|S1) or probaility of S2 given the state at S1. To wrap the Bayesian business up, we can compute the inference using Bayesian rule as follow\n\n$$\nP(A|B) = \\frac{P(B|A) P(A)}{P(B)}\n$$\n\nwhere \n\n- $$P(A\\mid B)$$: probability of state A given probability of B. \n- $$P(B\\mid A)$$: probability of state B given probability of A. \n- $$P(B)$$: Probaility of A being true( the given probability)\n- $$P(A)$$: Probability of B being true (the given probability)\n\nThese [notes](https://ermongroup.github.io/cs228-notes/representation/directed/) from cs228 course explain more in-depth on Baysian network.\n\nWhen dealing with HMM, each observable state will depend on one or multiple hidden state. \n- here, hidden state refer to state that are unseen or **hidden**. As an example, hidden state of a human being can be happy or sad. \n- observable state refering to state that can be seen or **observable**. For example, Crying or smiling\n\n### 3. Sumary of HMM Applications\n\nenough for the theory, let's now go over an sumarry of HMM in everyday actions. \n- Weather prediction\n- Speech recognition\n- Part of speech tagging\n- Music generation\n- Google search\n- Word type correction\n- Digital writing correction\n- Facial expression identification using in video\n- Human action recognition from Time Sequential Images\n- Biological Sequence Analysis.[link](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2766791/)\n- Creadit card fraud detection. [Link](https://thesai.org/Downloads/Volume7No2/Paper_5-Hidden_Markov_Models_HMMs_and_Security_Applications.pdf)\n- Filtering Sensor values.(Kalman filter)\n\n## II. Tools set for HMM\nWhen working with HMM, we will need some knowledge about the problem domain. As shown in Bayse's rule, to compute the **posterior**, $$P(A|B)$$ we need a **prior**, $$P(A)$$,  knowledge about the problem domain as well as the likelihood of the state in the sequence. \nIn HMM we will need Transition matrix(A), Emission matrix(B), and initial state matrix($\\pi$). \n- Transition matrix($A$): represent the probability of transitioning from a hidden state to other hidden state. \n- Emission matrix($B$): indicate the probability of an **emitted** being at a hidden state. \n - Initial state matrix($$\\pi$$): indicate the starting hidden state. As when we start a sequence, there are no previous state to look at. \n \nThe three matrix weight can be represent on graphical HMM model as below\n\u003ccenter\u003e\u003cimg src=\"https://user-images.githubusercontent.com/35666615/53783262-7ea24d00-3ed6-11e9-8a93-92c5e400cf5c.png\" height=\"330\" width=\"650\"\u003e\u003c/center\u003e \n\u003ccenter\u003e Graphical model of HMM\u003c/center\u003e\n\n\nTo generalize, what we just discussed. \n- $$A_{i,j}$$ indicate the transition state from one hidden state,i to next hidden state, j.\n- $$B_{i}(O_t)$$indicate the emission probability of emitting an observable state $$O_t$$ from hidden state at step t, denotes as i.\n- $$\\pi_i$$ indicate the initial probability of a hidden state, i. \n\n\n\n## III. HMM Aplications (HMM in actions) \nThere are far more and beyond application utilizing HMM technique. Let's discuss a couple to gain a more innovative look at HMM. \n\n- Weather prediction\n\t- Weather data source are gigantic as we can collect everyday. Well, I would argu that HMM cannot be a fully weather forecast predictor, but It can augment with other technique create by brillian minds meteorologist.\n\n- Speech recognition\n\t- Our language often follow a pattern. For example, a verb are more likely to come up after a noun or a noun often come after the word 'the'. Our speech often are represented as wave form signal. After sampling, we can retrieve discrete point which we can apply the HMM technique that we discuss above to analyze.\n- Part of speech tagging\n\t- From young age, we often start learning our language by breaking and analyzing part of speech. As part of the process we assigning if a sub-sequence is a verb or a noun, we often find pattern on what part of speech usually come after another. This task can be also accomplish with HMM. In Divya' [article](https://medium.freecodecamp.org/an-introduction-to-part-of-speech-tagging-and-the-hidden-markov-model-953d45338f24), she discussed over part of speech using HMM amazingly well. \n- Music generation\n- Google search\n\t- Of course, Our everyday favorite tool is base on a HMM model. Imagine searching for needles(our queries) in the world biggest haystack(the world wide web). The search would become exhaustive search. Luckily, Google came up with a way of assigning probability to link using [random surfer trick](http://pr.efactory.de/e-pagerank-algorithm.shtml). The first chapter of John Maccormick's book, 9 algorithm that change the future also walk in-depth over the PageRank algorithm. \n- Word type correction\n\t- Similar to speech recognition, on each word that user type, HMM model can also help predict the word that are likely to appear next, thus, this will help display option for user correct their writing. \n- Digital writing correction\n- Facial expression identification using in video\n- Human action recognition from Time Sequential Images\n- Biological Sequence Analysis.[link](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2766791/)\n- Creadit card fraud detection. [Link](https://thesai.org/Downloads/Volume7No2/Paper_5-Hidden_Markov_Models_HMMs_and_Security_Applications.pdf)\n- Filtering Sensor values.(Kalman filter)\n\t- One of my favorite application of HMM is the kalman filter. In discrete time, we have HMM. In the continuous world, we have the kalman filter. Kalman filter is mostly use to verify and extract sensory data. Kalman filter assign probability to a sensor value thus, help us tell how much to trust a sensor values. \n\n## IV. References\n\nhttps://ermongroup.github.io/cs228-notes/representation/directed/\nhttp://www.davidsbatista.net/blog/2017/11/11/HHM_and_Naive_Bayes/\nhttp://www.cs.umd.edu/~djacobs/CMSC828/ApplicationsHMMs.pdf\nhttps://medium.com/@postsanjay/hidden-markov-models-simplified-c3f58728caab"},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"hidden-markov-model-hmm-in-everyday-life"},"buildId":"Y66DXjpn3XfY2iHDxlRI4","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>